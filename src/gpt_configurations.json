{
    "GPT_CONFIG_124M": {
        "vocab_size": 50257,
        "context_length": 1024,
        "emb_dim": 768,
        "n_heads": 12,
        "n_layers": 12,
        "drop_rate": 0.1,
        "qkv_bias": false
    },
    "GPT_CONFIG_MICRO": {
        "vocab_size": 50257,
        "context_length": 256,
        "emb_dim": 384,
        "n_heads": 6,
        "n_layers": 6,
        "drop_rate": 0.1,
        "qkv_bias": false,
        "attention_type": "gqa",
        "num_kv_groups": 2
    },
    "GPT_CONFIG_SMALL_COLAB": {
        "vocab_size": 50257,
        "context_length": 256,
        "emb_dim": 256,
        "n_heads": 8,
        "n_layers": 8,
        "drop_rate": 0.1,
        "qkv_bias": false,
        "attention_type": "gqa",
        "num_kv_groups": 4
    },
    "GQA_CONFIG": {
        "vocab_size": 50257,           
        "context_length": 256,        
        "emb_dim": 256,                 
        "n_heads": 12,                   
        "n_layers": 12,                  
        "hidden_dim": 768,             
        "head_dim": 256,                 
        "qk_norm": true,                 
        "n_kv_groups": 4,                
        "rope_base": 1000000.0,       
        "dtype": "torch.bfloat16"
    }
}